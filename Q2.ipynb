{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(7)\n",
    "\n",
    "n_samples, n_features = 100, 200\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "k = 5\n",
    "# beta generated with k nonzeros\n",
    "#coef = 10 * np.random.randn(n_features)\n",
    "coef = 10 * np.ones(n_features)\n",
    "inds = np.arange(n_features)\n",
    "np.random.shuffle(inds)\n",
    "coef[inds[k:]] = 0  # sparsify coef\n",
    "y = np.dot(X, coef)\n",
    "\n",
    "# add noise\n",
    "y += 0.01 * np.random.normal((n_samples,))\n",
    "\n",
    "# Split data in train set and test set\n",
    "n_samples = X.shape[0]\n",
    "X_train, y_train = X[:25], y[:25]\n",
    "X_test, y_test = X[25:], y[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-3a88dc6cb7ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfeatures_r2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_r2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 222\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 200)"
     ]
    }
   ],
   "source": [
    "# 2.1\n",
    "# from sklearn.feature_selection import RFECV\n",
    "# from sklearn.linear_model import Lasso\n",
    "\n",
    "# selected_features = []\n",
    "\n",
    "# features_r2 = []\n",
    "\n",
    "# for i in range(0,201):\n",
    "#     clf = linear_model.LinearRegression()\n",
    "#     clf.fit(X_train[:,i].reshape(-1, 1), y_train)\n",
    "#     features_r2.append(r2_score(y_test, clf.predict(X_test)) )\n",
    "\n",
    "# print(features_r2)\n",
    "\n",
    "\n",
    "# alpha = 3\n",
    "# lasso = Lasso(alpha=alpha)\n",
    "\n",
    "# y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\n",
    "# r2_score_lasso = r2_score(y_test, y_pred_lasso)\n",
    "# print(\"R^2 on test data : %f\" % r2_score_lasso)\n",
    "\n",
    "\n",
    "# rfecv = RFECV(estimator=lasso, step=1, cv=5, scoring='accuracy')\n",
    "# rfecv = rfecv.fit(X_train, y_train)\n",
    "# print('Optimal number of features :', rfecv.n_features_)\n",
    "# print('Best features :', x_train.columns[rfecv.support_])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "0.181 (+/-0.823) for {'alpha': 0.0001}\n",
      "0.181 (+/-0.822) for {'alpha': 0.00013203517797162948}\n",
      "0.182 (+/-0.822) for {'alpha': 0.00017433288221999874}\n",
      "0.189 (+/-0.829) for {'alpha': 0.00023018073130224678}\n",
      "0.596 (+/-0.472) for {'alpha': 0.0003039195382313198}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0004012807031942776}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0005298316906283707}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0006995642156712634}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0009236708571873865}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0012195704601594415}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0016102620275609393}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0021261123338996556}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0028072162039411755}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0037065129109221566}\n",
      "1.000 (+/-0.000) for {'alpha': 0.004893900918477494}\n",
      "1.000 (+/-0.000) for {'alpha': 0.006461670787466976}\n",
      "1.000 (+/-0.000) for {'alpha': 0.008531678524172814}\n",
      "1.000 (+/-0.000) for {'alpha': 0.011264816923358867}\n",
      "1.000 (+/-0.000) for {'alpha': 0.014873521072935119}\n",
      "1.000 (+/-0.000) for {'alpha': 0.0196382800192977}\n",
      "1.000 (+/-0.000) for {'alpha': 0.02592943797404667}\n",
      "1.000 (+/-0.000) for {'alpha': 0.03423597957607583}\n",
      "1.000 (+/-0.000) for {'alpha': 0.04520353656360245}\n",
      "1.000 (+/-0.000) for {'alpha': 0.05968456995122311}\n",
      "1.000 (+/-0.000) for {'alpha': 0.07880462815669913}\n",
      "1.000 (+/-0.000) for {'alpha': 0.10404983103657853}\n",
      "1.000 (+/-0.000) for {'alpha': 0.1373823795883264}\n",
      "0.999 (+/-0.000) for {'alpha': 0.1813930693911063}\n",
      "0.999 (+/-0.001) for {'alpha': 0.2395026619987486}\n",
      "0.998 (+/-0.001) for {'alpha': 0.31622776601683794}\n",
      "The best alpha is: 0.0006995642156712634\n"
     ]
    }
   ],
   "source": [
    "# 2.3\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "lasso = Lasso(random_state=0, max_iter=10000)\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "n_folds = 5\n",
    "\n",
    "clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, scoring='r2')\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Best Alpha:\", clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "        \n",
    "print(\"The best alpha is: 0.0006995642156712634\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds:  1\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  2\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  3\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  4\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  5\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  6\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  7\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  8\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "Number of folds:  9\n",
      "Best Alpha: {'alpha': 0.0006995642156712634}\n",
      "As the number of folds increases, the hyper parameter stays the same.\n"
     ]
    }
   ],
   "source": [
    "# 2.4\n",
    "\n",
    "for n_fold in range(1,10):\n",
    "    print(\"Number of folds: \", n_fold)\n",
    "    clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, scoring='r2')\n",
    "    clf.fit(X, y)\n",
    "    print(\"Best Alpha:\", clf.best_params_)\n",
    "    \n",
    "print(\"As the number of folds increases, the hyper parameter stays the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5\n",
    "\n",
    "LassoCV will choose a set of alphas and perform comparison searches over then, using cross-validation to generate lasso models and internally score the results. The previous step used GridsearchCV + Lasso model, which is essentially performing the same functions as LassoCV. The results from LassoCV and GridsearchCV + Lasso should agree with each other if the parameters were configured the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

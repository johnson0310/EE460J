{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Random Forests to try to get the best possibletest accuracyon MNIST. This involves getting  acquainted  with  how  Random  Forests  work,  understanding  their  parameters,  andtherefore using Cross Validation to find the best settings.  How well can you do?  You shoulduse the accuracy metric, since this is what you used in Lab 5 â€“ therefore this will allow you tocompare your results from Random Forests with your results from L1- and L2- RegularizedLogistic Regression.  What are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'criterion': 'gini', 'n_estimators': 70}\n",
      "Score: 0.9636428571428571\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_features='log2', max_depth=20)\n",
    "                                \n",
    "param_grid = {\n",
    "'n_estimators':[x for x in range(10, 80, 10)],\n",
    "'criterion':['gini', 'entropy']\n",
    "}\n",
    "\n",
    "forest_CV = GridSearchCV(estimator=forest, param_grid=param_grid, cv=10)\n",
    "forest_CV.fit(X_train, y_train)\n",
    "print(\"best parameters: \" + str(forest_CV.best_params_))\n",
    "\n",
    "\n",
    "# prediction = []\n",
    "# for i in range(X_test.shape[0]):\n",
    "#     prediction.append(forest_CV.predict(X_test[i, :]))\n",
    "\n",
    "print(\"Score: \" + str(accuracy_score(y_test, forest_CV.predict(X_test))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use  Boosting  to  do  the  same.   Take  the  time  to  understand  how  XGBoost  works  (and/orother boosting packages available).  Try your best to tune your hyper-parameters.  As addedmotivation:  typically the winners and near-winners of the Kaggle competition are those that are best able to tune an cross validate XGBoost.  What are the hyperparameters of your bestmodel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 30, 'subsample': 0.2}\n",
      "0.8912857142857142\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "boosted_forest = xgb.XGBClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid_Boost = {\n",
    "    'n_estimators':[x for x in range(10,40,10)],\n",
    "    'max_depth':[x for x in range(1,5)],\n",
    "    'learning_rate':[.05],\n",
    "    'subsample':[.2]\n",
    "}\n",
    "\n",
    "forest_GS = GridSearchCV(estimator=boosted_forest, param_grid=param_grid_Boost, cv=5)\n",
    "forest_GS.fit(X_train, y_train, eval_metric='auc')\n",
    "print(forest_GS.best_params_)\n",
    "\n",
    "print(accuracy_score(y_test, forest_GS.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('CIFAR_10_small', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'criterion': 'entropy', 'n_estimators': 70}\n",
      "Score: 0.4056\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_features='log2', max_depth=20)\n",
    "                                \n",
    "param_grid = {\n",
    "'n_estimators':[x for x in range(10, 80, 10)],\n",
    "'criterion':['gini', 'entropy']\n",
    "}\n",
    "\n",
    "forest_CV = GridSearchCV(estimator=forest, param_grid=param_grid, cv=10)\n",
    "forest_CV.fit(X_train, y_train)\n",
    "\n",
    "print(\"best parameters: \" + str(forest_CV.best_params_))\n",
    "print(\"Score: \" + str(accuracy_score(y_test, forest_CV.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 30, 'subsample': 0.2}\n",
      "0.379\n"
     ]
    }
   ],
   "source": [
    "boosted_forest = xgb.XGBClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid_Boost = {\n",
    "    'n_estimators':[x for x in range(10,40,10)],\n",
    "    'max_depth':[x for x in range(1,5)],\n",
    "    'learning_rate':[.05],\n",
    "    'subsample':[.2]\n",
    "}\n",
    "\n",
    "forest_GS = GridSearchCV(estimator=boosted_forest, param_grid=param_grid_Boost, cv=5)\n",
    "forest_GS.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "print(forest_GS.best_params_)\n",
    "print(accuracy_score(y_test, forest_GS.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

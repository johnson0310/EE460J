{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
      "Requirement already satisfied: setuptools in /Users/davidrollins/opt/anaconda3/lib/python3.7/site-packages (from tika) (41.4.0)\n",
      "Requirement already satisfied: requests in /Users/davidrollins/opt/anaconda3/lib/python3.7/site-packages (from tika) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/davidrollins/opt/anaconda3/lib/python3.7/site-packages (from requests->tika) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/davidrollins/opt/anaconda3/lib/python3.7/site-packages (from requests->tika) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/davidrollins/opt/anaconda3/lib/python3.7/site-packages (from requests->tika) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/davidrollins/opt/anaconda3/lib/python3.7/site-packages (from requests->tika) (2.8)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tika: filename=tika-1.24-cp37-none-any.whl size=32884 sha256=069256ecab7c2ecb8496bcb4cb962b34329f6be1400b08e5c6600a6c03e34f78\n",
      "  Stored in directory: /Users/davidrollins/Library/Caches/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-1.24\n"
     ]
    }
   ],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "from tika import parser\n",
    "import random\n",
    "from random import choices\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Read Shannon’s 1948 paper ’A Mathematical Theory of Communication’.  Focus on pages 1-19 (upto Part II), the remaining part is more relevant for communication.http://math.harvard.edu/~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: \n",
    "#### Scraping, Entropy and ICML papers.ICML\n",
    "\n",
    "The International Conference on Machine Learning – is a top research conference in Machinelearning.   \n",
    "\n",
    "Scrape  all  the  pdfs  of  all  ICML  2020  papers  from http://proceedings.mlr.press/v119/\n",
    "\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "\n",
    "2.  Let Z be a randomly selected word in a randomly selected ICML paper.  Estimate the entropy of Z.\n",
    "\n",
    "3.  Synthesize a random paragraph using the marginal distribution over words.\n",
    "\n",
    "4.  (Optional)  Synthesize  a  random  paragraph  using  an  n-gram  model  on  words.   Synthesizea random paragraph using any model you want.  Top five synthesized text paragraphs winbonus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-06 10:28:41,557 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/abbati19a/abbati19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-abbati19a-abbati19a.pdf.\n",
      "2021-03-06 10:28:42,529 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/abels19a/abels19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-abels19a-abels19a.pdf.\n",
      "2021-03-06 10:28:43,756 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/abu-el-haija19a/abu-el-haija19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-abu-el-haija19a-abu-el-haija19a.pdf.\n",
      "2021-03-06 10:28:44,041 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/acharya19a/acharya19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-acharya19a-acharya19a.pdf.\n",
      "2021-03-06 10:28:44,326 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/acharya19b/acharya19b.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-acharya19b-acharya19b.pdf.\n",
      "2021-03-06 10:28:44,790 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/acharya19c/acharya19c.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-acharya19c-acharya19c.pdf.\n",
      "2021-03-06 10:28:45,172 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/adams19a/adams19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-adams19a-adams19a.pdf.\n",
      "2021-03-06 10:28:45,815 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/adel19a/adel19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-adel19a-adel19a.pdf.\n",
      "2021-03-06 10:28:46,759 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/adiga19a/adiga19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-adiga19a-adiga19a.pdf.\n",
      "2021-03-06 10:28:47,591 [MainThread  ] [INFO ]  Retrieving http://proceedings.mlr.press/v97/agarwal19a/agarwal19a.pdf to /var/folders/c_/ns1rsjk10x1cszkmwzj6_9w40000gq/T/v97-agarwal19a-agarwal19a.pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'and', 'a', 'in', '1', 'to', 'is', 'for', 'we', 'that', '2', '0', 'on', 'with', 'al', 'et', 'learning', 'this', 'as', 'k', 'are', 'p', 's', 'x', 'by', 'i', 'n', 'from', 'w']\n",
      "['model', 'communication', 'which', 'each', 'using', 'where', 'distribution', '2018', 'data', 'not']\n"
     ]
    }
   ],
   "source": [
    "url = \"http://proceedings.mlr.press/v97/\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, 'html5lib')\n",
    "#print(soup.prettify()) -- too long\n",
    "\n",
    "preamble = \"http://proceedings.mlr.press/v97/\"\n",
    "papers = []\n",
    "\n",
    "\n",
    "for elm in soup.find_all(attrs={\"target\":\"_blank\"}):  # soup() is equivalent to soup.find_all()\n",
    "    href = elm[\"href\"]\n",
    "    \n",
    "    if preamble in href and \"supp.pdf\" not in href:\n",
    "        papers.append(href)\n",
    "    \n",
    "#print(papers)\n",
    "\n",
    "\n",
    "words = []\n",
    "for pdf in papers[:10]: # remove [:10] in final version - takes a long time to execute\n",
    "    content = parser.from_file(pdf)[\"content\"]\n",
    "    words += re.sub(\"[^\\w]\", \" \", content).split()\n",
    "\n",
    "    \n",
    "    \n",
    "count = {}\n",
    "for word in words:\n",
    "    if word.lower() not in count:\n",
    "        count[word.lower()] = 1\n",
    "    else:\n",
    "        count[word.lower()] += 1\n",
    "        \n",
    "\n",
    "def find_highest(count):\n",
    "    max_key = max(count.items(), key=operator.itemgetter(1))[0]\n",
    "    count.pop(max_key, None)\n",
    "    return max_key\n",
    "\n",
    "top30_words = []\n",
    "for i in range(0,30):\n",
    "    top30_words.append(find_highest(count))\n",
    "    \n",
    "    \n",
    "print(top30_words)\n",
    "\n",
    "\n",
    "uninteresting_words = ['the',\n",
    " 'and',\n",
    " 'of',\n",
    " 'a',\n",
    " 'to',\n",
    " 'in',\n",
    " 'for',\n",
    " '0',\n",
    " 'is',\n",
    " 'we',\n",
    " '1',\n",
    " 't',\n",
    " 'on',\n",
    " 'by',\n",
    " 'as',\n",
    " 's',\n",
    " 'this',\n",
    " 'are',\n",
    " 'with',\n",
    " '2',\n",
    " 'that',\n",
    " 'be',\n",
    " 'et',\n",
    " 'from',\n",
    " 'al',\n",
    " '3',\n",
    " 'can',\n",
    "  'an',\n",
    "  'cn',\n",
    "   'w',\n",
    "    'it',\n",
    "    'g','e','o','our','der','at','z','q','x','m','http','org','4','d','j','y','c','v','5','b']\n",
    "\n",
    "top10_words = []\n",
    "while(len(top10_words) < 10):\n",
    "    \n",
    "    word = find_highest(count)\n",
    "    \n",
    "    if word in uninteresting_words:\n",
    "        continue\n",
    "    else:\n",
    "        top10_words.append(word)\n",
    "    \n",
    "    \n",
    "print(top10_words)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.423910708210517\n"
     ]
    }
   ],
   "source": [
    "entropy = 0\n",
    "\n",
    "rand_word = random.randint(0, len(words))\n",
    "rand_paper = random.randint(0, len(papers))\n",
    "total_words = sum(count.values())\n",
    "for word, num_word in count.items():\n",
    "    \n",
    "    word_probability = num_word/total_words\n",
    "    entropy += (-1)*word_probability*np.log2(word_probability)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound proving transformed right however respect nontriviality following test homophily matrices latent past gorbach 89 system less hypothesis 2017 replay relative assume activation optimality quite graphs error invariant posterior more layer spread classical delta group figure privileging call coin flawed\n"
     ]
    }
   ],
   "source": [
    "probabilities = []\n",
    "choice_words = []\n",
    "for word, num in count.items():\n",
    "    choice_words.append(word)\n",
    "    probabilities.append(num/total_words)\n",
    "    \n",
    "random_words = choices(choice_words, probabilities, k=40)\n",
    "paragraph = ' '.join(random_words)\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
